---
title: "MLCM Replication Notebook"
output:
  html_document:
    df_print: paged
  html_notebook: default
---

## "The Machine Learning Control Method for Counterfactual Forecasting"

## Augusto Cerqua, Marco Letta, Fiammetta Menchetti

## The Impact of the COVID-19 Crisis on Education in Italy

This Notebook replicates the key results of the empirical application reported in Section 4 of the paper "The Machine Learning Control Method for Counterfactual Forecasting" (Cerqua et al., 2023) using the publicly available data that can be downloaded [here](https://github.com/marclet/MLCM-Replication-Notebook/tree/main/data). 

Note that, for the sake of computational speed, the main dataset ("dataset_replication_full_FINAL.csv") only includes the restricted set of relevant variables selected by the preliminary random forest (see Cerqua et al., 2023 for details). In the same folder, you can find the file "metadata_replication.xlsx", which contains detailed metadata regarding the variables included in both datasets. 

The notebook should help to show the underlying code for researchers interested in replicating our analysis or in adapting it to new datasets for their own applications.

The explanations in this notebook are kept short. Please refer to the paper for the full details regarding the application.

Click on the "Code" button on the top right of this notebook and then click on "Download Rmd" to extract the underlying code.

Be aware that running the analyses in this notebook with the original number of bootstrap replications (standard errors in the paper are computed based on 1,000 replications) will take a long time. For this reason, the code below has the option "nboot" set to 50 in all instances. Change it to "nboot = 1000" if you also want to get the same standard errors as in the paper.

<br>
<br>

## Data preparation

Download the main data from [here](https://github.com/marclet/MLCM-Replication-Notebook/tree/main/data).

```{r message = FALSE, warning = FALSE}
rm(list=ls())
# Load required libraries
library(CAST)
library(caret)
library(gbm)
library(elasticnet)
library(pls)
library(stats)
library(utils)
library(randomForest)
library(rpart)
library(MachineControl)
```

We start by loading and preparing the data:

```{r}
# Download dataset from https://github.com/marclet/MLCM-Replication-Notebook/tree/main/data into your working directory and load the raw data 
# Full dataset
wd <- "C:\\Users\\39380\\Documents\\GitHub\\MLCM-Replication-Notebook\\docs\\"
source(paste0(wd, "/Functions/package.R"))
setwd(wd)
dataset_full <- read.csv(paste0(wd, "/data/dataset_replication_full_FINAL.csv"), na.strings=c("",".","NA"))
x.cate <- read.csv(paste0(wd, "/data/CATE_dataset.csv"))
# Pre-intervention dataset
dataset_pre_2020 <- subset(dataset_full, year<2020) 
# Intervention date
int_date <- 2020

# Comment: this is a 579 X 4 panel dataset (i.e., the number of units in the panel is 579
#          the number of times is 4). The dataset is organized in a a long manner
#          i.e., it has 579 X 4 = 2316 rows, one column for ID variable ('id'), 
#          one column for the time variable ('year') and one column for the dependent 
#          variable ('std_math_score'). The dataset also contains 154 potential 
#          explanatory variables. 

```

<br>
<br>

# Panel cross-validation

This step runs the horse-race competition among the four selected ML algorithms (LASSO, Partial Least Squares, boosting, and random forest) and tunes their hyperparameters on their pre-treatment data. This is the chunk code we used to produce Panel A, Table I (performances in terms of MSE) and Figure 3 (distribution of forecasting errors) in the paper.

```{r}
# Set seed
set.seed(1)

num_cov <- 20 # we keep the 20 most predictive variables selected by the preliminary random forest

### 2.2. Setting the tuning parameters of the Machine Learning algorithms 
# SGB
gbmGrid <-  expand.grid(interaction.depth = c(1, 2), 
                        n.trees=c(1000, 2000), 
                        shrinkage = c(0.001, 0.002, 0.005),
                        n.minobsinnode = c(5, 10))
bo <- list(method = "gbm",
           tuneGrid = gbmGrid)
# RF
rf <- list(method = "rf",
           ntree = 1000)
# LASSO
lassoGrid <- expand.grid(fraction = seq(0.1, 0.9, by = 0.1))
lasso <- list(method = "lasso",
              tuneGrid = lassoGrid,
              preProc = c("center", "scale"))
# PLS
pls <- list(method = "pls")

### 2.3. Applying Panel Cross Validation to select the best importance threshold and the best
###      performing algorithm based on the RMSE criterion. Note that we include here the whole dataset
###      because the 'PanelCrossValidation' function subsets the data internally via the argument 
###      'int_date'. See lines 202 and beyond of 'package.R'.
PCV <- lapply(num_cov, FUN = function(x){
  
  total_columns <- ncol(dataset_full)
  xvar <- colnames(dataset_full)[(total_columns-19):total_columns] ;
  data <- as.PanelMLCM(y = dataset_full[, "std_math_score"], 
                       timevar = dataset_full[, "year"], 
                       id = dataset_full[, "id"],
                       x = dataset_full[, xvar]) ;
  rfGrid <- expand.grid(mtry = round(c(ncol(data)/2, ncol(data)/3, ncol(data)/4))) ;
  plsGrid <- expand.grid(ncomp = seq(1, (ncol(data)-2), by = 1)) ;
  rf$tuneGrid <- rfGrid ;
  pls$tuneGrid <- plsGrid ;
  PCV <- PanelCrossValidation(data = data, int_date = int_date, ML_methods = list(bo, rf, lasso, pls)) ;
  return(PCV)
})
PCV

metrics <- sapply(PCV, FUN = function(x)(x$best.metric))
ind <- which(metrics == min(metrics))
best <- PCV[[ind]]$best
best
```

## Estimation of causal effects

Now that we know the best-performing model, we can proceed with the estimation of individual, average, and conditional average treatment effects and export them to produce Figure 4 (map of individual causal effects) and Figure 5 (data-driven CATEs) reported in the paper.

```{r}
### 3.1. Definition of the final dataset, based on the selected variable importance threshold
# Set seed
set.seed(1)

total_columns <- ncol(dataset_full)
xvar <- colnames(dataset_full)[(total_columns-19):total_columns]
data <- as.PanelMLCM(y = dataset_full[, "std_math_score"], 
                     timevar = dataset_full[, "year"], 
                     id = dataset_full[, "id"],
                     x = dataset_full[, xvar]) 

### 3.2. Causal effect estimation
causal_effects <- MLCM(data = data, int_date = int_date, inf_type = "block", PCV = best,
                       nboot = 1, CATE = TRUE, x.cate = x.cate)

# ATE
ate_eff <- c(causal_effects$ate, causal_effects$conf.ate)
names(ate_eff) <- c("ATE", "Lower_bound", "Upper_bound")
ate_eff
# Individual effects
ind_eff <- data.frame(causal_effects$ind.effects)
names(ind_eff) <- c("ID", "IND_EFF")
# CATE
cate_eff <- causal_effects$cate.inf
cate_eff
causal_effects$cate

# now export the results
write.csv(ind_eff, file = paste0(wd, "/Output/individual_effects.csv"))
write.csv(ate_eff, file = paste0(wd, "/Output/ate.csv"))
write.csv(cate_eff, file = paste0(wd, "/Output/cate.csv"))
```

<br>
<br>

## Placebo test

Finally, we repeat the analysis on the pre-pandemic years (2018 and 2019) to compute placebo treatment effects and export them to produce Panel B of Table III, Figure 2, and Figure I reported in Online Appendix Part 3.

```{r}
# Set seed
set.seed(1)

# 2019 #
### 3.1. Definition of the final dataset, based on the selected variable importance threshold
dataset_full_2019 <- subset(dataset_full, year<2020) 
int_date_2019 <- 2019
data_placebo_2019 <- as.PanelMLCM(y = dataset_full_2019[, "std_math_score"], 
                                  timevar = dataset_full_2019[, "year"], 
                                  id = dataset_full_2019[, "id"],
                                  x = dataset_full_2019[, xvar]) 

effects_placebo_2019 <- MLCM(data = data_placebo_2019, int_date = int_date_2019, inf_type = "block", PCV = best,
                             nboot = 1, CATE = FALSE)

# ATE (Placebo 2019)
ate_eff_placebo_2019 <- c(effects_placebo_2019$ate, effects_placebo_2019$conf.ate)
names(ate_eff_placebo_2019) <- c("ATE", "Lower_bound", "Upper_bound")
ate_eff_placebo_2019
# Individual effects (Placebo 2019)
ind_eff_placebo_2019 <- data.frame(effects_placebo_2019$ind.effects)
names(ind_eff_placebo_2019) <- c("ID", "IND_EFF")

write.csv(ind_eff_placebo_2019, file = paste0(wd, "/Output/individual_effects_placebo_2019.csv"))
write.csv(ate_eff_placebo_2019, file = paste0(wd, "/Output/ate_placebo_2019.csv"))




# 2018 #
### 3.1. Definition of the final dataset, based on the selected variable importance threshold
dataset_full_2018 <- subset(dataset_full, year<2019) 
int_date_2018 <- 2018
data_placebo_2018 <- as.PanelMLCM(y = dataset_full_2018[, "std_math_score"], 
                                  timevar = dataset_full_2018[, "year"], 
                                  id = dataset_full_2018[, "id"],
                                  x = dataset_full_2018[, xvar]) 

effects_placebo_2018 <- MLCM(data = data_placebo_2018, int_date = int_date_2018, inf_type = "block", PCV = best,
                             nboot = 1, CATE = FALSE)

# ATE (Placebo 2018)
ate_eff_placebo_2018 <- c(effects_placebo_2018$ate, effects_placebo_2018$conf.ate)
names(ate_eff_placebo_2018) <- c("ATE", "Lower_bound", "Upper_bound")
ate_eff_placebo_2018
# Individual effects (Placebo 2018)
ind_eff_placebo_2018 <- data.frame(effects_placebo_2018$ind.effects)
names(ind_eff_placebo_2018) <- c("ID", "IND_EFF")

# export the results
write.csv(ind_eff_placebo_2018, file = paste0(wd, "/Output/individual_effects_placebo_2018.csv"))
write.csv(ate_eff_placebo_2018, file = paste0(wd, "/Output/ate_placebo_2018.csv"))
```
